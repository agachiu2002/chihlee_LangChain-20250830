{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b16844",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOllama' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     21\u001b[0m chat_prompt_template \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(complex_template)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# --- 模型改為 gemma3:1b ---\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 初始化 Ollama 模型\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 請確保您的 Ollama 服務正在運行，且 'gemma3:1b' 模型已拉取\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m ollama_model \u001b[38;5;241m=\u001b[39m \u001b[43mChatOllama\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma3:1b\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:11434\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 準備要傳遞給模板的變數\u001b[39;00m\n\u001b[0;32m     29\u001b[0m translation_variables \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_language\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m法文\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_language\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m繁體中文\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m藝術\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe stock market experienced a significant downturn due to global economic uncertainties.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatOllama' is not defined"
     ]
    }
   ],
   "source": [
    "# 從 LangChain 導入必要的模組\n",
    "from langchain_ollama import OllamaLLM # 導入 OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate # 仍使用 ChatPromptTemplate 構建模板\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # 載入 .env 檔案中的環境變數 (如果存在的話)\n",
    "\n",
    "# 定義您的多變數翻譯模板\n",
    "complex_template = \"\"\"\n",
    "你是一位專業的{target_language}翻譯家，專精於{domain}領域。\n",
    "請將以下{source_language}文本翻譯成{target_language}，並確保：\n",
    "1. 保持原文的語氣和風格\n",
    "2. 使用專業術語\n",
    "3. 符合{target_language}的語言習慣\n",
    "\n",
    "{source_language}文本：{text}\n",
    "{target_language}翻譯：\n",
    "\"\"\"\n",
    "# 創建 ChatPromptTemplate\n",
    "chat_prompt_template = ChatPromptTemplate.from_template(complex_template)\n",
    "\n",
    "# --- 模型改為 gemma3:1b ---\n",
    "# 初始化 Ollama 模型\n",
    "# 請確保您的 Ollama 服務正在運行，且 'gemma3:1b' 模型已拉取\n",
    "ollama_model = ChatOllama(model=\"gemma3:1b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# 準備要傳遞給模板的變數\n",
    "translation_variables = {\n",
    "    \"source_language\": \"法文\",\n",
    "    \"target_language\": \"繁體中文\",\n",
    "    \"domain\": \"藝術\",\n",
    "    \"text\": \"The stock market experienced a significant downturn due to global economic uncertainties.\"\n",
    "}\n",
    "\n",
    "# 使用 chat_prompt_template.format_messages() 格式化提示為 LangChain Message 物件\n",
    "# ChatOllama.invoke 期望接收一個 Message 物件列表\n",
    "formatted_messages = chat_prompt_template.format_messages(**translation_variables)\n",
    "\n",
    "print(\"=== 格式化後的 LangChain Message 物件 ===\")\n",
    "for msg in formatted_messages:\n",
    "    print(f\"Type: {type(msg).__name__}, Content: {msg.content}\")\n",
    "\n",
    "# 呼叫 Ollama 模型進行翻譯\n",
    "print(\"\\n=== 呼叫 Ollama 模型進行翻譯 ===\")\n",
    "try:\n",
    "    # 直接將格式化後的 Message 列表傳遞給 invoke\n",
    "    translation_result = ollama_model.invoke(formatted_messages)\n",
    "\n",
    "    print(\"\\n=== Ollama 翻譯結果 (invoke) ===\")\n",
    "    print(translation_result.content)\n",
    "except Exception as e:\n",
    "    print(f\"Ollama 呼叫失敗：{e}\")\n",
    "    print(\"請確認 Ollama 服務正在運行，且 'gemma3:1b' 模型已成功拉取。\")\n",
    "\n",
    "# 您也可以使用 LangChain Expression Language (LCEL) 來鏈接提示和模型，這更簡潔：\n",
    "print(\"\\n=== 使用 LCEL 鏈接提示與模型進行翻譯 ===\")\n",
    "translation_chain = chat_prompt_template | ollama_model\n",
    "\n",
    "try:\n",
    "    lcel_result = translation_chain.invoke(translation_variables)\n",
    "    print(\"\\n=== LCEL 翻譯結果 ===\")\n",
    "    print(lcel_result.content)\n",
    "except Exception as e:\n",
    "    print(f\"LCEL 翻譯失敗：{e}\")\n",
    "    print(\"請確認 Ollama 服務正在運行，且 'gemma3:1b' 模型已成功拉取。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
