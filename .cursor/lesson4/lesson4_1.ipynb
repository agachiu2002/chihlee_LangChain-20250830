{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e3113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "所有答案\n",
      "content='台灣國旗的三大顏色是紅、白和藍。' additional_kwargs={} response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-09-20T06:38:38.4447426Z', 'done': True, 'done_reason': 'stop', 'total_duration': 801700900, 'load_duration': 84935800, 'prompt_eval_count': 36, 'prompt_eval_duration': 237098000, 'eval_count': 17, 'eval_duration': 479159000, 'model_name': 'llama3.2:latest'} id='run--b1ad92d4-b331-44a1-8123-3c492f21ced5-0' usage_metadata={'input_tokens': 36, 'output_tokens': 17, 'total_tokens': 53}\n",
      "回答內容是\n",
      "台灣國旗的三大顏色是紅、白和藍。\n"
     ]
    }
   ],
   "source": [
    "#ollama api\n",
    "# Chat Model Documents:https://python.langchain.com/docs/integrations/chat/\n",
    "# Google Chat Model Documents:https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "# Create a ChatOllama model\n",
    "# 透過網址的方式連結ollama (指定 base_url 指向 Ollama server)\n",
    "# 預設 Ollama server 在本機的 11434 埠，若在其他主機或埠請改成相對應的網址\n",
    "\n",
    "#model = ChatOllama(model=\"llama3.2:latest\", base_url=\"http://host.docker.internal:11434\")\n",
    "model = ChatOllama(model=\"llama3.2:latest\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "\n",
    "# Invoke the model with a message\n",
    "result = model.invoke(\"台灣國旗有哪三種顏色?\")\n",
    "print(type(result))\n",
    "print(\"所有答案\")\n",
    "print(result)\n",
    "print(\"回答內容是\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1719a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###googele-gemini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "194fe7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "所有答案\n",
      "content='81除以9的答案是 **9**。' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--89097781-d725-403f-b142-777b900283bc-0' usage_metadata={'input_tokens': 10, 'output_tokens': 67, 'total_tokens': 77, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 55}}\n",
      "回答內容是\n",
      "81除以9的答案是 **9**。\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Create a ChatGoogleGenerativeAI model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "#Invoke the model with a message\n",
    "result = model.invoke(\"81除以9的答案是?\")\n",
    "print(type(result))\n",
    "print(\"所有答案\")\n",
    "print(result)\n",
    "print(\"回答內容是\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8749ef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_google_genai.chat_models.ChatGoogleGenerativeAI'>\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "所有答案\n",
      "content='## 聲聲月台\\n\\n月台，時間的渡口，\\n空氣中，先是人聲的低語，\\n嗡嗡，嗡嗡，如蜂群盤旋。\\n腳步，沓沓，沓沓，匆匆劃過地磚。\\n\\n遠方，一聲嗚——\\n汽笛穿透薄暮，劃開靜默。\\n由遠及近的轟隆轟隆，\\n鐵獸在軌道上甦醒。\\n\\n喀嚓喀嚓，鋼輪咬合著冰冷的軌道，\\n金屬的顫音，漸次放大。\\n然後，一聲漫長的嘶——\\n巨大的氣流，在月台邊緣嘆息。\\n哐噹！車門開啟，又一扇世界敞開。\\n\\n滴——嘟——，廣播聲甜膩而清晰，\\n提示著旅人，歸鄉與遠行。\\n行李箱，咕嚕咕嚕，\\n載滿期盼，拖曳過人海。\\n擁抱，低語，以及來不及說的再見。\\n\\n哐噹！車門再度咬合，\\n又一聲嗚——\\n催促著，告別著，漸行漸遠。\\n轟隆轟隆，這次是心跳，\\n跳動在逐漸模糊的視野。\\n\\n喀嚓喀嚓，鋼輪再次規律地敲擊，\\n將所有故事，拉入速度的漩渦。\\n只留下月台，空曠而沉默。\\n然而，那些擬聲的迴盪，\\n依然在空氣中盤旋。\\n\\n嗚—— 轟隆轟隆—— 嘶—— 哐噹！\\n滴——嘟—— 沓沓—— 咕嚕咕嚕——\\n它們是記憶的序曲，\\n也是故事的終章。\\n在無數次的聲響中，\\n織成火車站永恆的流動與等待。' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--16fdaaa2-14d6-4540-a496-cbc43d3eb2a0-0' usage_metadata={'input_tokens': 14, 'output_tokens': 1977, 'total_tokens': 1991, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1578}}\n",
      "回答內容是\n",
      "## 聲聲月台\n",
      "\n",
      "月台，時間的渡口，\n",
      "空氣中，先是人聲的低語，\n",
      "嗡嗡，嗡嗡，如蜂群盤旋。\n",
      "腳步，沓沓，沓沓，匆匆劃過地磚。\n",
      "\n",
      "遠方，一聲嗚——\n",
      "汽笛穿透薄暮，劃開靜默。\n",
      "由遠及近的轟隆轟隆，\n",
      "鐵獸在軌道上甦醒。\n",
      "\n",
      "喀嚓喀嚓，鋼輪咬合著冰冷的軌道，\n",
      "金屬的顫音，漸次放大。\n",
      "然後，一聲漫長的嘶——\n",
      "巨大的氣流，在月台邊緣嘆息。\n",
      "哐噹！車門開啟，又一扇世界敞開。\n",
      "\n",
      "滴——嘟——，廣播聲甜膩而清晰，\n",
      "提示著旅人，歸鄉與遠行。\n",
      "行李箱，咕嚕咕嚕，\n",
      "載滿期盼，拖曳過人海。\n",
      "擁抱，低語，以及來不及說的再見。\n",
      "\n",
      "哐噹！車門再度咬合，\n",
      "又一聲嗚——\n",
      "催促著，告別著，漸行漸遠。\n",
      "轟隆轟隆，這次是心跳，\n",
      "跳動在逐漸模糊的視野。\n",
      "\n",
      "喀嚓喀嚓，鋼輪再次規律地敲擊，\n",
      "將所有故事，拉入速度的漩渦。\n",
      "只留下月台，空曠而沉默。\n",
      "然而，那些擬聲的迴盪，\n",
      "依然在空氣中盤旋。\n",
      "\n",
      "嗚—— 轟隆轟隆—— 嘶—— 哐噹！\n",
      "滴——嘟—— 沓沓—— 咕嚕咕嚕——\n",
      "它們是記憶的序曲，\n",
      "也是故事的終章。\n",
      "在無數次的聲響中，\n",
      "織成火車站永恆的流動與等待。\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Create a ChatGoogleGenerativeAI model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=1) #temperure0~1\n",
    "print(type(model))\n",
    "\n",
    "#Invoke the model with a message\n",
    "result = model.invoke(\"請用中文創作一首以火車站擬聲的新詩\")\n",
    "print(type(result))\n",
    "print(\"所有答案\")\n",
    "print(result)\n",
    "print(\"回答內容是\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7c0a0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 載入環境變數（可用 OLLAMA_URL / OLLAMA_MODEL 覆蓋預設）\n",
    "load_dotenv()\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "MODEL_NAME = os.getenv(\"OLLAMA_MODEL\", \"llama3.2:latest\")\n",
    "\n",
    "# 使用最原始的呼叫方式：直接以字串 prompt 送到 Ollama\n",
    "model = ChatOllama(model=MODEL_NAME, base_url=OLLAMA_URL)\n",
    "\n",
    "\n",
    "def answer(prompt: str) -> str:\n",
    "    \"\"\"最簡單的 wrapper：把 prompt 傳給 model.invoke，回傳文字回應。\"\"\"\n",
    "    if not prompt:\n",
    "        return \"\"\n",
    "    res = model.invoke(prompt)\n",
    "    return res.content if hasattr(res, \"content\") else str(res)\n",
    "\n",
    "\n",
    "# 最小的 Gradio 介面：一個輸入框 + 一個文字輸出\n",
    "iface = gr.Interface(\n",
    "    fn=answer,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"在此輸入問題，按送出...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Ollama 簡易 Gradio 範例\",\n",
    "    description=\"示範如何把原先的 Ollama 呼叫整合到 Gradio。可用 OLLAMA_URL/OLLAMA_MODEL 環境變數覆蓋預設。\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(server_name=\"0.0.0.0\", server_port=7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
