# LangChain é–‹ç™¼å®¹å™¨

é€™æ˜¯ä¸€å€‹å°ˆç‚º LangChain é–‹ç™¼è¨­è¨ˆçš„ Docker å®¹å™¨ç’°å¢ƒï¼ŒåŒ…å«äº†æ‰€æœ‰å¿…è¦çš„å·¥å…·å’Œæœå‹™ã€‚

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æ–¹æ³• 1ï¼šä½¿ç”¨ Docker Composeï¼ˆæ¨è–¦ï¼‰

```bash
# æ§‹å»ºä¸¦å•Ÿå‹•å®¹å™¨
docker-compose up --build

# åœ¨èƒŒæ™¯é‹è¡Œ
docker-compose up -d --build
```

### æ–¹æ³• 2ï¼šä½¿ç”¨ Docker

```bash
# æ§‹å»ºæ˜ åƒ
docker build -t langchain-dev .

# é‹è¡Œå®¹å™¨
docker run -it -p 8888:8888 -p 11434:11434 -v $(pwd):/app langchain-dev
```

### æ–¹æ³• 3ï¼šä½¿ç”¨ VS Code Dev Containers

1. å®‰è£ VS Code Dev Containers æ“´å±•
2. æ‰“é–‹å°ˆæ¡ˆè³‡æ–™å¤¾
3. æŒ‰ `Ctrl+Shift+P`ï¼Œé¸æ“‡ "Dev Containers: Reopen in Container"

## ğŸ“‹ åŒ…å«çš„æœå‹™

- **Python 3.12** - ä¸»è¦é–‹ç™¼ç’°å¢ƒ
- **Jupyter Lab** - äº’å‹•å¼é–‹ç™¼ç’°å¢ƒ (ç«¯å£ 8888)
- **Ollama** - æœ¬åœ° LLM æœå‹™ (ç«¯å£ 11434)
- **LangChain** - AI æ‡‰ç”¨é–‹ç™¼æ¡†æ¶
- **FastAPI** - Web API æ¡†æ¶
- **Gradio** - å¿«é€Ÿ UI å»ºæ§‹å·¥å…·

## ğŸ”§ é å®‰è£çš„æ¨¡å‹

å®¹å™¨å•Ÿå‹•æ™‚æœƒè‡ªå‹•ä¸‹è¼‰ä»¥ä¸‹æ¨¡å‹ï¼š

- `gemma:2b` - Google çš„è¼•é‡ç´šæ¨¡å‹
- `llama3.2:1b` - Meta çš„é«˜æ•ˆæ¨¡å‹
- `qwen2.5:1.5b` - é˜¿é‡Œå·´å·´çš„å¤šèªè¨€æ¨¡å‹

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
.
â”œâ”€â”€ Dockerfile              # Docker æ˜ åƒå®šç¾©
â”œâ”€â”€ docker-compose.yml      # å®¹å™¨ç·¨æ’é…ç½®
â”œâ”€â”€ .devcontainer/          # VS Code é–‹ç™¼å®¹å™¨é…ç½®
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ requirements.txt        # Python ä¾è³´
â”œâ”€â”€ start-container.sh      # å®¹å™¨å•Ÿå‹•è…³æœ¬
â”œâ”€â”€ .dockerignore          # Docker å¿½ç•¥æª”æ¡ˆ
â””â”€â”€ README.md              # èªªæ˜æ–‡ä»¶
```

## ğŸŒ è¨ªå•æœå‹™

- **Jupyter Lab**: http://localhost:8888
- **Ollama API**: http://localhost:11434

## ğŸ’¡ ä½¿ç”¨ç¯„ä¾‹

### åœ¨ Jupyter ä¸­ä½¿ç”¨ Ollama

```python
from ollama import chat

response = chat(model='gemma:2b', messages=[
    {
        'role': 'user',
        'content': 'Hello, how are you?',
    },
])

print(response['message']['content'])
```

### ä½¿ç”¨ LangChain èˆ‡ Ollama

```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="gemma:2b")
response = llm.invoke("What is LangChain?")
print(response)
```

## ğŸ› ï¸ é–‹ç™¼å·¥å…·

å®¹å™¨å…§é è£äº†ä»¥ä¸‹é–‹ç™¼å·¥å…·ï¼š

- **Black** - Python ç¨‹å¼ç¢¼æ ¼å¼åŒ–
- **Flake8** - ç¨‹å¼ç¢¼æª¢æŸ¥
- **Pytest** - æ¸¬è©¦æ¡†æ¶
- **Git** - ç‰ˆæœ¬æ§åˆ¶

## ğŸ“ å¸¸ç”¨å‘½ä»¤

```bash
# æŸ¥çœ‹é‹è¡Œä¸­çš„å®¹å™¨
docker-compose ps

# æŸ¥çœ‹å®¹å™¨æ—¥èªŒ
docker-compose logs -f

# åœæ­¢å®¹å™¨
docker-compose down

# é‡æ–°æ§‹å»ºå®¹å™¨
docker-compose up --build

# é€²å…¥å®¹å™¨
docker-compose exec langchain-dev bash

# æŸ¥çœ‹ Ollama æ¨¡å‹
docker-compose exec langchain-dev ollama list

# ä¸‹è¼‰æ–°æ¨¡å‹
docker-compose exec langchain-dev ollama pull <model-name>
```

## ğŸ” æ•…éšœæ’é™¤

### Ollama æœå‹™ç„¡æ³•å•Ÿå‹•
```bash
# æª¢æŸ¥ Ollama ç‹€æ…‹
docker-compose exec langchain-dev ollama list

# é‡å•Ÿ Ollama æœå‹™
docker-compose exec langchain-dev pkill ollama
docker-compose exec langchain-dev ollama serve &
```

### Jupyter Lab ç„¡æ³•è¨ªå•
- ç¢ºèªç«¯å£ 8888 æœªè¢«å ç”¨
- æª¢æŸ¥é˜²ç«ç‰†è¨­å®š
- å˜—è©¦ä½¿ç”¨ `http://127.0.0.1:8888`

### æ¨¡å‹ä¸‹è¼‰å¤±æ•—
```bash
# æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹
docker-compose exec langchain-dev ollama pull gemma:2b
```

## ğŸ“š æ›´å¤šè³‡æº

- [LangChain å®˜æ–¹æ–‡æª”](https://python.langchain.com/)
- [Ollama å®˜æ–¹æ–‡æª”](https://ollama.com/docs)
- [Docker å®˜æ–¹æ–‡æª”](https://docs.docker.com/)
- [VS Code Dev Containers](https://code.visualstudio.com/docs/remote/containers)
